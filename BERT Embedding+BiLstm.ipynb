{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "wired-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm,trange\n",
    "from seqeval.scheme import IOB2\n",
    "from seqeval.metrics import classification_report,f1_score\n",
    "from transformers import *\n",
    "from sadice import SelfAdjDiceLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "foreign-campaign",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data_train = pd.read_csv(\"IOB2_Data/BC_train_IOB2_all.txt\",sep = '\\t', na_filter=False)\n",
    "data_dev = pd.read_csv(\"IOB2_Data/BC_dev_IOB2_all.txt\",sep = '\\t', na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "incorporated-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                     s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence#\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[self.n_sent]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "assisted-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = SentenceGetter(data_train)\n",
    "dev_getter = SentenceGetter(data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "organizational-samuel",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [[word[0] for word in sentence] for sentence in getter.sentences]\n",
    "dev_sentences = [[word[0] for word in sentence] for sentence in dev_getter.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "enormous-journey",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [[s[1] for s in sent] for sent in getter.sentences]\n",
    "dev_labels = [[s[1] for s in sent] for sent in dev_getter.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "appointed-regression",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-', 'O', 'I-', 'PAD']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_values = list(set(data_train[\"Tag\"].values))\n",
    "tag_values.append(\"PAD\")\n",
    "tag_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "disciplinary-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "manual-productivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "        if n_subwords > 1 and 'B-' in label:\n",
    "            labels.extend([label])\n",
    "            _ = 'I-' + label.split('B-')[1]\n",
    "            if _ not in tag_values:\n",
    "                tag_values.append(_)\n",
    "                print(_)\n",
    "            labels.extend([_] * (n_subwords-1))\n",
    "        else:\n",
    "            labels.extend([label] * n_subwords)\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "historic-landing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(sentences, labels)\n",
    "]\n",
    "print('done')\n",
    "dev_tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(dev_sentences, dev_labels)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "covered-tucson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-', 1: 'O', 2: 'I-', 3: 'PAD'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
    "idx2tag = {i: t for i, t in enumerate(tag_values)}\n",
    "idx2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "southeast-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
    "tokenized_labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]\n",
    "\n",
    "dev_tokenized_texts = [token_label_pair[0] for token_label_pair in dev_tokenized_texts_and_labels]\n",
    "dev_tokenized_labels = [token_label_pair[1] for token_label_pair in dev_tokenized_texts_and_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "strategic-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts]\n",
    "dev_input_ids = [tokenizer.convert_tokens_to_ids(txt) for txt in dev_tokenized_texts]\n",
    "tags = [[tag2idx.get(l) for l in lab] for lab in tokenized_labels]\n",
    "dev_tags = [[tag2idx.get(l,tag2idx['O']) for l in lab] for lab in dev_tokenized_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "horizontal-facility",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class BertNerDataset(Dataset):\n",
    "    def __init__(self,sentences,labels, word_pad_idx, tag_pad_idx, max_len = 500):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.word_pad_idx = word_pad_idx\n",
    "        self.tag_pad_idx = tag_pad_idx\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.sentences[index],self.labels[index])\n",
    "        \n",
    "    def collate_fn(self, datasets):\n",
    "        sentences = [dataset[0] for dataset in datasets]\n",
    "        labels = [dataset[1] for dataset in datasets]\n",
    "        max_sent = max([len(data) for data in sentences])\n",
    "        max_len = max([min(len(sentence), self.max_len) for sentence in sentences])\n",
    "        pad_sentence = []\n",
    "        pad_label = []\n",
    "        for sentence,label in zip(sentences,labels):\n",
    "            \n",
    "            if len(sentence) > max_len:\n",
    "#                 print('asd')\n",
    "                pad_sentence.append(sentence[:max_len])\n",
    "                pad_label.append(label[:max_len])\n",
    "                attention_masks = [[float(i != 0.0) for i in ii] for ii in pad_sentence]\n",
    "            else:\n",
    "#                 print('zxc')\n",
    "                pad_sentence.append(sentence+[self.word_pad_idx]*(max_len-len(sentence)))\n",
    "                pad_label.append(label+[self.tag_pad_idx]*(max_len-len(label)))\n",
    "                attention_masks = [[float(i != 0.0) for i in ii] for ii in pad_sentence]\n",
    "        return torch.LongTensor(pad_sentence), torch.LongTensor(pad_label),torch.tensor(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "synthetic-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "bs = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 15\n",
    "max_grad_norm = 3.0\n",
    "\n",
    "tr_dataset = BertNerDataset(input_ids,tags,tokenizer.convert_tokens_to_ids('[PAD]'),tag2idx['PAD'])\n",
    "va_dataset = BertNerDataset(dev_input_ids,dev_tags,tokenizer.convert_tokens_to_ids('[PAD]'),tag2idx['PAD'])\n",
    "\n",
    "train_dataloader = DataLoader(tr_dataset, batch_size=bs,\n",
    "                            collate_fn=tr_dataset.collate_fn)\n",
    "valid_dataloader = DataLoader(va_dataset, batch_size=bs,\n",
    "                            collate_fn=va_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "blind-execution",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class EmbeddedRnn(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_vocab, n_layer=1):\n",
    "        super(EmbeddedRnn, self).__init__()\n",
    "        self.n_layer = n_layer\n",
    "        self.embedding_size = 3072\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bert = BertForTokenClassification.from_pretrained('bert-base-cased',output_hidden_states =True)\n",
    "#         self.bert.eval()\n",
    "        self.lstm = nn.LSTM(self.embedding_size, hidden_dim, num_layers=n_layer,batch_first = True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(2 * hidden_dim, output_vocab)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def embedding(self,x,att_mask):\n",
    "        sent_embeddings = []\n",
    "        _,hidden_states = self.bert(x,att_mask)\n",
    "        token_embeddings = torch.stack(hidden_states[:-1],dim = 0)\n",
    "#         with torch.no_grad():\n",
    "        token_embeddings = token_embeddings.permute(1,2,0,3)\n",
    "        for tks in token_embeddings:\n",
    "            token_vecs = []\n",
    "            for tk in tks:\n",
    "                cat_vec = torch.cat((tk[-1] , tk[-2] , tk[-3] , tk[-4]) , dim = 0)\n",
    "                token_vecs.append(cat_vec)\n",
    "            token_vecs = torch.stack(token_vecs , 0)\n",
    "            sent_embeddings.append(token_vecs)\n",
    "        sent_embeddings = torch.stack(sent_embeddings , 0)\n",
    "        return sent_embeddings    \n",
    "\n",
    "    def forward(self, x, x_att):\n",
    "        embedded = self.embedding(x,x_att)\n",
    "#         print(embedded.shape)\n",
    "        output, hidden = self.lstm(embedded)\n",
    "#         print(output.shape)\n",
    "        output = self.fc1(output)\n",
    "        output = self.softmax(output)\n",
    "        return output,hidden\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        hidden = Variable(torch.zeros(2 * self.n_layer, batch_size, self.hidden_dim))\n",
    "        cell = Variable(torch.zeros(2 * self.n_layer, batch_size, self.hidden_dim))\n",
    "        return [hidden, cell]\n",
    "#         return hidden\n",
    "\n",
    "model = EmbeddedRnn(300 , len(tag2idx))\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "protected-present",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=5, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "quiet-behavior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "valid\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=3e-5,\n",
    "    eps=1e-8\n",
    ")\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "all_dataloader = {\n",
    "    'train' : train_dataloader,\n",
    "    'valid' : valid_dataloader,\n",
    "    }\n",
    "for i in all_dataloader:\n",
    "    print(i)\n",
    "\n",
    "# criterion = SelfAdjDiceLoss(reduction=\"none\")\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = FocalLoss(gamma = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "greater-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup,get_cosine_schedule_with_warmup\n",
    "\n",
    "epochs = 20\n",
    "TAG_PAD_IDX = tag2idx['PAD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-feature",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss : 0.0001313879938827759\n",
      "0.09859154929577464\n",
      "\n",
      "loss : 0.00014161245200378631\n",
      "0.1732283464566929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|███▉                                                                           | 1/20 [19:45<6:15:21, 1185.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.00      0.00      0.00         0\n",
      "           _       0.21      0.21      0.21       105\n",
      "\n",
      "   micro avg       0.15      0.21      0.17       105\n",
      "   macro avg       0.10      0.10      0.10       105\n",
      "weighted avg       0.21      0.21      0.21       105\n",
      "\n",
      "\n",
      "loss : 1.7610060100560087e-05\n",
      "0.6313645621181263\n",
      "\n",
      "loss : 0.00012007343481244592\n",
      "0.4545454545454546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|███████▉                                                                       | 2/20 [39:35<5:56:25, 1188.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.00      0.00      0.00         0\n",
      "           _       0.54      0.62      0.58       105\n",
      "\n",
      "   micro avg       0.36      0.62      0.45       105\n",
      "   macro avg       0.27      0.31      0.29       105\n",
      "weighted avg       0.54      0.62      0.58       105\n",
      "\n",
      "\n",
      "loss : 8.77713249665112e-06\n",
      "0.7250996015936255\n",
      "\n",
      "loss : 3.269035245436711e-05\n",
      "0.6437768240343347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|███████████▊                                                                   | 3/20 [59:27<5:37:07, 1189.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.00      0.00      0.00         0\n",
      "           _       0.64      0.71      0.67       105\n",
      "\n",
      "   micro avg       0.59      0.71      0.64       105\n",
      "   macro avg       0.32      0.36      0.34       105\n",
      "weighted avg       0.64      0.71      0.67       105\n",
      "\n",
      "\n",
      "loss : 5.260741506245309e-06\n",
      "0.7857142857142856\n",
      "\n",
      "loss : 4.52675972896867e-05\n",
      "0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|███████████████▍                                                             | 4/20 [1:19:17<5:17:20, 1190.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.00      0.00      0.00         0\n",
      "           _       0.72      0.83      0.77       105\n",
      "\n",
      "   micro avg       0.69      0.83      0.75       105\n",
      "   macro avg       0.36      0.41      0.39       105\n",
      "weighted avg       0.72      0.83      0.77       105\n",
      "\n",
      "\n",
      "loss : 8.587155102479045e-06\n",
      "0.7229862475442043\n",
      "\n",
      "loss : 5.335020009655144e-06\n",
      "0.7906976744186046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|███████████████████▎                                                         | 5/20 [1:39:01<4:56:57, 1187.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.00      0.00      0.00         0\n",
      "           _       0.78      0.81      0.79       105\n",
      "\n",
      "   micro avg       0.77      0.81      0.79       105\n",
      "   macro avg       0.39      0.40      0.40       105\n",
      "weighted avg       0.78      0.81      0.79       105\n",
      "\n",
      "\n",
      "loss : 3.141350787784917e-06\n",
      "0.886128364389234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "records = {\n",
    "    'loss':[],\n",
    "    'F1':[],\n",
    "}\n",
    "model.train(True)\n",
    "f_slist = []\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for loader in all_dataloader:\n",
    "        train_loss,valid_loss = [],[]\n",
    "        print('')\n",
    "        predictions , true_labels , x_  = [],[],[]\n",
    "        for x, y ,x_attn in all_dataloader[loader]:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            x_attn = x_attn.to(device)\n",
    "            hidden = model.initHidden(x.size(0))\n",
    "            hidden[0] = hidden[0].to(device)\n",
    "            hidden[1] = hidden[1].to(device)\n",
    "            asd = y.shape[1]\n",
    "            output, hidden = model(x, x_attn)\n",
    "            predictions.extend(np.argmax(output.detach().cpu().numpy(), axis=2))\n",
    "            for i in y.detach().cpu().numpy():\n",
    "                _ = []\n",
    "                for j in i:\n",
    "                    if j != TAG_PAD_IDX:\n",
    "                        _.append(idx2tag[j])\n",
    "                true_labels.append(_)\n",
    "\n",
    "            output = output.reshape(-1,output.shape[-1])\n",
    "            y = y.reshape(-1)\n",
    "    #             print(output.shape,y.shape)\n",
    "            \n",
    "            loss = criterion(output,y)\n",
    "#             loss = loss.reshape(-1, asd).mean(-1).mean()\n",
    "#             if loader == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_loss.append(loss.cpu().item()) \n",
    "        print(f'loss : {np.mean(np.array(train_loss))}')\n",
    "        pred_tags = [[idx2tag[p_i] for p_i, l_i in zip(p, l) if l_i != \"PAD\"] for p, l in zip(predictions, true_labels)]\n",
    "        f_ = f1_score(true_labels,pred_tags, scheme = IOB2)\n",
    "        print(f_)\n",
    "        if loader == 'valid':\n",
    "            f_slist.append(f_)\n",
    "            print(classification_report(true_labels,pred_tags , scheme = IOB2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adjacent-mauritius",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.51, 0.47, 0.44, 0.6 , 0.58, 0.06, 0.55, 0.52, 0.61, 0.58,\n",
       "       0.66, 0.62, 0.59, 0.21, 0.59, 0.55, 0.54, 0.55, 0.54])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.array(f_slist) ,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adapted-locator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-', 'I-', 'O', 'PAD'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([p for pred in pred_tags for p in pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "economic-closure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0.7968442440032959 max_error: tensor(4.7684e-07, device='cuda:0')\n",
      "torch.Size([128, 1000, 8, 4]) torch.Size([128, 8, 4])\n",
      "time: 0.023962020874023438 max_error: tensor(9.5367e-07, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "maxe = 0\n",
    "for i in range(1000):\n",
    "    x = torch.rand(12800,2)*random.randint(1,10)\n",
    "    x = Variable(x.cuda())\n",
    "    l = torch.rand(12800).ge(0.1).long()\n",
    "    l = Variable(l.cuda())\n",
    "    print()\n",
    "    output0 = FocalLoss(gamma=0)(x,l)\n",
    "    output1 = nn.CrossEntropyLoss()(x,l)\n",
    "    a = output0.data\n",
    "    b = output1.data\n",
    "    if abs(a-b)>maxe: maxe = abs(a-b)\n",
    "print('time:',time.time()-start_time,'max_error:',maxe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-africa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
