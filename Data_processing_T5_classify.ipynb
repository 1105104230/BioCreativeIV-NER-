{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceramic-patch",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ken19980727\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from transformers import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "living-prisoner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>predict_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm 37 weeks pregnant so I can do whatever the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@RaynaJaye bahahahahahahahaha</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Mamas_RipDad thank yu 😘</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bruh do you wanna fight nah man it's problem</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It has been a difficult transition .</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37893</th>\n",
       "      <td>@doritmi @KathyMcGrath4 Got my Tdap at 37 week...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37894</th>\n",
       "      <td>Oaklee's heartbeat was 142 today,and I have fi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37895</th>\n",
       "      <td>Got my whooping cough vaccine yesterday and no...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37896</th>\n",
       "      <td>Got myself a new bff! I'm now sat on my bed on...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37897</th>\n",
       "      <td>So I'm just about to drink castor oil again be...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37898 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             source_text  target_text  \\\n",
       "0      I'm 37 weeks pregnant so I can do whatever the...            0   \n",
       "1                          @RaynaJaye bahahahahahahahaha            0   \n",
       "2                               @Mamas_RipDad thank yu 😘            0   \n",
       "3           Bruh do you wanna fight nah man it's problem            0   \n",
       "4                   It has been a difficult transition .            0   \n",
       "...                                                  ...          ...   \n",
       "37893  @doritmi @KathyMcGrath4 Got my Tdap at 37 week...            1   \n",
       "37894  Oaklee's heartbeat was 142 today,and I have fi...            1   \n",
       "37895  Got my whooping cough vaccine yesterday and no...            1   \n",
       "37896  Got myself a new bff! I'm now sat on my bed on...            1   \n",
       "37897  So I'm just about to drink castor oil again be...            1   \n",
       "\n",
       "       predict_text  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  \n",
       "...             ...  \n",
       "37893             1  \n",
       "37894             1  \n",
       "37895             1  \n",
       "37896             1  \n",
       "37897             1  \n",
       "\n",
       "[37898 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kai = pd.read_csv('bc7_task3_valid.input0.tsv',sep = '\\t')\n",
    "df_me = pd.read_csv('Non-Process_data/BioCreative_ValTask3.tsv',sep = '\\t')\n",
    "df_kai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "reduced-principal",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-497e8adacf80>:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df_kai[df_kai['predict_text'] == 1][df_kai['target_text'] == 1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>predict_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37805</th>\n",
       "      <td>Just get on birth control and use two condoms....</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37806</th>\n",
       "      <td>I have a possible infection from my Zofran pum...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37807</th>\n",
       "      <td>@bethanygiuffre it's time for the epidural!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37809</th>\n",
       "      <td>*puts cocoa butter on, takes prenatal vitamin,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37810</th>\n",
       "      <td>Update: I got the epidural, back contractions ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37893</th>\n",
       "      <td>@doritmi @KathyMcGrath4 Got my Tdap at 37 week...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37894</th>\n",
       "      <td>Oaklee's heartbeat was 142 today,and I have fi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37895</th>\n",
       "      <td>Got my whooping cough vaccine yesterday and no...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37896</th>\n",
       "      <td>Got myself a new bff! I'm now sat on my bed on...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37897</th>\n",
       "      <td>So I'm just about to drink castor oil again be...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             source_text  target_text  \\\n",
       "37805  Just get on birth control and use two condoms....            1   \n",
       "37806  I have a possible infection from my Zofran pum...            1   \n",
       "37807        @bethanygiuffre it's time for the epidural!            1   \n",
       "37809  *puts cocoa butter on, takes prenatal vitamin,...            1   \n",
       "37810  Update: I got the epidural, back contractions ...            1   \n",
       "...                                                  ...          ...   \n",
       "37893  @doritmi @KathyMcGrath4 Got my Tdap at 37 week...            1   \n",
       "37894  Oaklee's heartbeat was 142 today,and I have fi...            1   \n",
       "37895  Got my whooping cough vaccine yesterday and no...            1   \n",
       "37896  Got myself a new bff! I'm now sat on my bed on...            1   \n",
       "37897  So I'm just about to drink castor oil again be...            1   \n",
       "\n",
       "       predict_text  \n",
       "37805             1  \n",
       "37806             1  \n",
       "37807             1  \n",
       "37809             1  \n",
       "37810             1  \n",
       "...             ...  \n",
       "37893             1  \n",
       "37894             1  \n",
       "37895             1  \n",
       "37896             1  \n",
       "37897             1  \n",
       "\n",
       "[79 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kai[df_kai['predict_text'] == 1][df_kai['target_text'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sound-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "senior = [i for i in df_me['text']]\n",
    "me = [i for i in df_kai['source_text']]\n",
    "\n",
    "for idx,(i,j) in enumerate(zip(senior,me)):\n",
    "    if i != j:\n",
    "        if i.strip() in me:\n",
    "            pass\n",
    "        else:\n",
    "            print(idx)\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-dispatch",
   "metadata": {},
   "source": [
    "### ========================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "criminal-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ty = 'bert-base-cased'\n",
    "# model_ty = 'bert-base-multilingual-cased'\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "tokenizer = BertTokenizer.from_pretrained(model_ty, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "suited-freeze",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL = torch.load('Model/BERT_case_BC.pt')\n",
    "with open('Model/tag_map.json') as jsonfile:\n",
    "    tag2idx = json.load(jsonfile)\n",
    "MODEL.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "professional-packing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'I-', 1: 'O', 2: 'B-', 3: 'PAD'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2tag = {tag2idx[t]:t for t in tag2idx}\n",
    "idx2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "noted-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,sent,o_sent):\n",
    "    torch_sent = torch.LongTensor(torch.tensor([sent])).cuda()\n",
    "    with torch.no_grad():\n",
    "        output = model(torch_sent)[0][0]\n",
    "        out_tag = [idx2tag[o] for o in torch.max(output,1)[1].cpu().numpy()]\n",
    "    pred = []\n",
    "    for s,t in zip(o_sent,out_tag):\n",
    "        if s == '<ETY>':\n",
    "            pred.append(t)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "common-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_pred = []\n",
    "tags_pred = []\n",
    "def first_wordpicee(w_p):\n",
    "    w_p[0] = '<ETY>'\n",
    "    return w_p\n",
    "    \n",
    "for i in df_kai.index:\n",
    "    tokens = nltk.word_tokenize(df_kai['source_text'][i])\n",
    "    if df_kai['predict_text'][i] == 1:\n",
    "        SENT = []\n",
    "        O_SENT = []\n",
    "        for token in tokens:\n",
    "            O_SENT.extend(first_wordpicee(tokenizer.tokenize(token)))\n",
    "            SENT.extend(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(token)))\n",
    "        tag = predict(MODEL,SENT,O_SENT)\n",
    "        tags_pred.append(tag)\n",
    "    else:\n",
    "        tags_pred.append(len(tokens)*['O'])\n",
    "    sentences_pred.append(tokens)\n",
    "# sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "stock-slovenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent,tag in zip(sentences_pred,tags_pred):\n",
    "    if len(sent) != len(tag):\n",
    "        print('asd')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "whole-packet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IOB2_me = pd.read_csv(\"IOB2_Data/BC_dev_IOB2_all.txt\",sep = '\\t')\n",
    "IOB2_me = IOB2_me.fillna('NA')\n",
    "type(IOB2_me['Sentence#'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "postal-monroe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Length</th>\n",
       "      <th>Sentence#</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'m</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weeks</td>\n",
       "      <td>O</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pregnant</td>\n",
       "      <td>O</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548096</th>\n",
       "      <td>another</td>\n",
       "      <td>O</td>\n",
       "      <td>7</td>\n",
       "      <td>37897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548097</th>\n",
       "      <td>day</td>\n",
       "      <td>O</td>\n",
       "      <td>3</td>\n",
       "      <td>37897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548098</th>\n",
       "      <td>I</td>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "      <td>37897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548099</th>\n",
       "      <td>'m</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "      <td>37897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548100</th>\n",
       "      <td>tired</td>\n",
       "      <td>O</td>\n",
       "      <td>5</td>\n",
       "      <td>37897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>548101 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word Tag  Length  Sentence#\n",
       "0              I   O       1          0\n",
       "1             'm   O       2          0\n",
       "2             37   O       2          0\n",
       "3          weeks   O       5          0\n",
       "4       pregnant   O       8          0\n",
       "...          ...  ..     ...        ...\n",
       "548096   another   O       7      37897\n",
       "548097       day   O       3      37897\n",
       "548098         I   O       1      37897\n",
       "548099        'm   O       2      37897\n",
       "548100     tired   O       5      37897\n",
       "\n",
       "[548101 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IOB2_me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "accepting-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_true = []\n",
    "tags_true = []\n",
    "s_num = 0\n",
    "s_ = []\n",
    "t_ = []\n",
    "for i in IOB2_me.index:\n",
    "    if s_num != IOB2_me['Sentence#'][i]:\n",
    "        s_num = IOB2_me['Sentence#'][i]\n",
    "        sentences_true.append(s_)\n",
    "        tags_true.append(t_)\n",
    "        s_ = []\n",
    "        t_ = []\n",
    "        s_.append(IOB2_me['Word'][i])\n",
    "        t_.append(IOB2_me['Tag'][i])\n",
    "    else:\n",
    "        s_.append(IOB2_me['Word'][i])\n",
    "        t_.append(IOB2_me['Tag'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "south-plasma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37897"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "active-allah",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "predicts = []\n",
    "## 還未增加NA轉nan的狀態\n",
    "for idt,s_t in enumerate(sentences_true):\n",
    "    if s_t in sentences_pred:\n",
    "        for idp,s_p in enumerate(sentences_pred):\n",
    "            if s_t == s_p:\n",
    "                labels.append(tags_true[idt])\n",
    "                predicts.append(tags_pred[idp])\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "therapeutic-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.scheme import IOB2\n",
    "from seqeval.metrics import classification_report,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "baking-shoot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           _       0.90      0.58      0.70        99\n",
      "\n",
      "   micro avg       0.90      0.58      0.70        99\n",
      "   macro avg       0.90      0.58      0.70        99\n",
      "weighted avg       0.90      0.58      0.70        99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels , predicts,scheme = IOB2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "pediatric-evaluation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asd\n",
      "asd\n",
      "['Ugh', '.', 'My', 'next', 'syringe', 'change', 'is', 'going', 'to', 'be', 'around', '4', 'am', '.', 'Whyyy', '.', '#', 'LifeWithA', 'Zofran', 'Pump']\n",
      "['@', 'daysmadeofnow', 'can', 'you', 'do', 'the', 'ibuprofen', '/', 'acetaminophen', 'switching', '?']\n",
      "['\\U000feb15I', 'hate', 'the', 'sleepy', 'side', 'effect', 'when', 'your', 'eggo', 'is', 'preggo', 'or', 'maybe', 'it', 'these', 'vitamins', '\\U000feb14never', 'wear', 'panties', 'ever', 'but', 'I', '...', 'http', ':', '//t.co/vQnof2HBx3']\n",
      "asd\n",
      "['Type', '1', 'diabetes', '&', 'amp', ';', 'pregnancy', '...', 'She', 'has', 'a', 'really', 'funky', 'placenta', 'that', 'requires', 'the', 'most', 'ridiculous', 'dose', 'of', 'insulin', '.', '’', 'http', ':', '//t.co/piwBDkYkvd']\n",
      "['Holy', 'shit', 'I', 'need', 'my', 'freaking', 'anxiety', 'meds', '😩']\n"
     ]
    }
   ],
   "source": [
    "def replace_NA(x):\n",
    "    if x == 'NA':\n",
    "        return 'nan'\n",
    "    else:\n",
    "        return x\n",
    "error_sent = [] \n",
    "for i,j in zip(sentences_true , sentences_pred):\n",
    "    if i != j:\n",
    "        if i in sentences_pred:\n",
    "            pass\n",
    "        else:\n",
    "            if list(map(replace_NA , i)) in sentences_pred:\n",
    "                print('asd')\n",
    "            else:\n",
    "                print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "greater-simon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'November', 'phone', 'bill', 'is', 'gon', 'na', 'be', 'so', 'high', '!', 'Ugh', 'activation', 'fees', 'lol'] 961\n",
      "['@', 'RetMSgt', 'I', 'used', 'to', 'have', 'this', 'pain', 'in', 'my', 'neck', 'a', 'few', 'times', 'a', 'year', '.', 'It', \"'s\", 'so', 'horrible', 'and', 'I', 'feel', 'for', 'anyone', 'who', 'has', 'to', 'go', 'through', 'it', '.', 'Ugh', '.'] 1113\n",
      "['Waiting', 'waiting', '.', 'Ugh'] 2465\n",
      "['If', 'My', 'Aunt', 'Do', \"n't\", 'Give', 'Me', 'The', 'Key', 'To', 'Get', 'In', 'The', 'House', '!', '!', '!', 'Ugh'] 2749\n",
      "['Why', 'am', 'I', 'so', 'tired', '?', 'I', 'hardly', 'did', 'anything', 'today', '!', 'Ugh', '.'] 3778\n",
      "['Is', 'my', 'brother', 'being', 'a', 'fckn', 'hoe', 'again', '?', 'Ugh'] 3787\n",
      "['Ugh', 'this', 'weather', 'change', 'and', 'my', 'sinuses', 'and', 'allergies', 'are', 'not', 'getting', 'along😭😭😭'] 4118\n",
      "['Ugh', 'so', 'miserable'] 5206\n",
      "['@', 'CheyCornwell', 'The', 'Way', 'My', 'Body', 'Is', 'Set', 'Up', 'Right', 'Now', 'Im', 'Ready', 'For', 'Her', 'To', 'Be', 'Here', '.', 'This', 'Crap', 'Is', 'Miserable', 'On', 'Top', 'Of', 'Shit', 'Thats', 'Going', 'On', 'Ugh'] 5648\n",
      "['Ugh', ',', 'there', 'are', 'some', 'days', 'I', 'can', 'not', 'help', 'myself', '.', 'https', ':', '//t.co/ErluaMYD5l'] 5748\n",
      "['Ugh', 'I', 'hate', 'it', 'when', 'M', 'cries', 'and', 'his', 'dad', 'does', \"n't\", 'know', 'how', 'to', 'console', 'him', 'at', 'all', '..', 'He', 'just', 'watches', 'him', 'cry', '😒'] 6150\n",
      "['@', 'UNLVgirl', 'I', 'know', '!', 'Ugh', '.'] 7107\n",
      "['Ugh', 'I', \"'ve\", 'gotten', 'barely', 'any', 'sleep', 'the', 'past', 'few', 'days', 'and', 'my', 'stuff', 'keeps', 'breaking', '#', 'StressedOut', '#', 'sleeplessnights'] 8446\n",
      "['@', 'visuelle_adams', 'Ugh', '!', 'I', 'Ca', \"n't\", 'Imagine', '!', '!', 'Last', 'Night', 'I', 'Wanted', 'To', 'Just', 'Cry', '.', 'She', 'Thinks', 'I', \"'m\", 'Starting', 'To', 'Dilate', ',', 'So', 'Tuesday', 'Imma', 'Tell', 'the', 'Doc', '.'] 9286\n",
      "['Ugh', 'I', 'want', 'a', 'redbull', 'so', 'bad', '...', '😦'] 9531\n",
      "['Ugh', '.', 'I', 'do', \"n't\", 'wan', 'na', 'feel', 'like', 'this', 'anymore', '😔😞😣😩😫'] 10427\n",
      "['Body', 'hurtin', '😩😣😔😞', '.', 'Ugh', '.', 'Hate', 'this'] 10739\n",
      "[\"Jaz'Alynn\", 'Will', 'You', 'Please', 'Move', 'A', 'Little', '?', 'Ugh', 'I', 'Wan', 'na', 'Cry', '!'] 10785\n",
      "['Ugh', 'why', 'are', 'all', 'these', 'gorgeous', 'celebs', 'cutting', 'off', 'their', 'hair', '?', '!', 'Stop', '!', '🙅'] 11600\n",
      "['Ugh', 'I', \"'m\", 'sick', 'and', 'do', 'not', 'want', 'to', 'be', 'at', 'school', '-_-'] 12095\n",
      "['Ugh', 'to', 'mad', 'ion', 'get', 'off', 'at', '9'] 12704\n",
      "['Ugh'] 13005\n",
      "['Ugh', 'I', \"'m\", 'so', 'annoyed', 'by', 'my', 'damn', 'hair', '!'] 13366\n",
      "['Ugh', 'now', 'my', 'stomach', 'is', 'gettin', 'tighter', '.', 'Man', 'ca', \"n't\", 'these', 'months', 'go', 'by', 'faster', '.', 'I', 'just', 'wan', 'na', 'give', 'birth', 'already'] 14128\n",
      "['The', 'nurses', 'keep', 'telling', 'me', 'not', 'to', 'sit', 'up', 'so', 'much', 'bc', 'than', 'baby', 'comes', 'off', 'the', 'monitor', '.', 'Ugh', '.'] 14340\n",
      "['Wish', 'I', 'could', 'sleep', '.', 'Ugh', '.', '😔'] 14439\n",
      "['Feelin', 'sick', '.', 'Light', 'headin', '.', 'Vomitin', '.', 'Ugh', '.', 'I', 'hate', 'these', 'feelings', 'again', '.'] 14781\n",
      "['I', 'just', 'want', 'my', 'back', 'to', 'stop', 'hurtin', '.', 'Ugh'] 16158\n",
      "['@', 'rameisha_', 'your', 'so', 'right', 'but', 'my', 'back', 'been', 'hurting', 'so', 'bad', '.', 'Sleep', 'pattern', 'is', 'horrible', '.', 'Ugh'] 16609\n",
      "['Ugh', 'Imma', 'Keep', 'Bitching', 'Til', 'I', 'Leave', '!', 'I', 'Just', 'Wan', 'na', 'Go', 'Home', '.', '😞😢'] 16761\n",
      "['I', 'Seen', 'A', 'Guy', 'The', 'Other', 'Day', 'Eating', 'Alone', '...', 'I', \"'m\", 'About', 'To', 'Be', 'Him', '..', 'Ugh', '.', 'Oh', 'Well', 'I', \"'ll\", 'Smash'] 16872\n",
      "['Starting', 'to', 'feel', 'worse.😞', 'Ugh', ',', 'I', 'hate', 'hate', 'hate', 'being', 'sick', '..'] 17469\n",
      "['I', 'hate', 'when', 'people', 'use', 'women', 'for', 'woman', '....', 'Ugh', '😒'] 17774\n",
      "['Ugh', 'I', 'hate', 'that', 'August', '13', 'I', 'have', 'to', 'go', 'to', 'Fountain', 'Vally', 'for', 'a', 'NT', 'Ultrasound', 'babe', 'better', 'leave', 'the', 'red', 'car', 'like', 'I', \"'m\", 'serious', '😂'] 18002\n",
      "['Ugh', ',', 'my', 'hands', 'are', 'so', 'swollen', 'I', 'ca', \"n't\", 'wear', 'my', 'wedding', 'ring', '😩'] 18525\n",
      "['Ugh', 'nigga', 'fuck', 'you', 'for', 'cuttin', 'a', 'tree', 'outside', 'my', 'window', '!', 'I', 'did', \"n't\", 'go', 'to', 'bed', 'until', '5', '...'] 18821\n",
      "['So', 'in', 'love', 'with', 'Superman', '.', 'Ugh', '!', '😍'] 18840\n",
      "['Ugh', '.', 'Getting', 'ready', 'for', 'these', '12', 'hour', 'shifts', '.', '😭🙇🏻'] 18857\n",
      "['Ugh', '.', 'I', 'keep', 'going', 'in', 'and', 'out', 'of', 'sleep', '.', 'Why', '?', '!', '😔'] 19653\n",
      "['Ugh', 'omg', 'she', 'just', 'melts', 'me', '!', '!', '!', '!', 'https', ':', '//t.co/9N8YoAr2IL'] 20299\n",
      "['Ugh', 'he', 'upsets', 'my', 'life', '!', '!', '!', '!', '!', '!', '!'] 20512\n",
      "['I', \"'m\", 'so', 'tired', 'of', 'these', 'allergies', '..', 'And', 'I', 'had', 'to', 'reschedule', 'my', 'prenatal', 'appointment', '..', 'Ugh', '.', 'I', 'just', 'want', 'to', 'see', 'an', 'ultrasound', '!', '!', '!', '!'] 20538\n",
      "['@', 'kheelopay', 'Ugh', 'Nisha', 'lost', 'my', 'phone', 'so', 'I', 'have', 'to', 'buy', 'another', 'POS', '😒'] 20547\n",
      "['Ugh', ',', 'im', 'like', 'FUCK', 'SCHOOL'] 20671\n",
      "['Ugh', ',', 'I', 'ca', \"n't\", 'sleep', '😭😭'] 20748\n",
      "['@', 'mrsjenryan', 'Ugh', 'the', 'panic', 'feeling', 'is', 'the', 'worst', '.', 'That', 'was', 'me', 'today', 'after', 'a', 'bath', 'and', 'when', 'they', 'did', 'come', 'off', '...', 'I', 'said', 'they', 'were', 'off', 'for', 'good', '!'] 20840\n",
      "['So', 'I', 'finish', 'all', 'my', 'shopping', 'and', 'get', 'home', 'to', 'see', 'that', 'bath', '&', 'amp', ';', 'body', 'is', 'having', 'another', 'amazing', 'sale', 'on', 'candles', '.', 'Ugh', 'back', 'to', 'the', 'mall', 'I', 'go', '!'] 21737\n",
      "['Ugh', 'why', 'do', 'I', 'have', 'to', 'work', '?', ':', '('] 21968\n",
      "['Me', ':', 'Ugh', ',', 'why', 'do', 'I', 'feel', 'so', 'gross', 'today', '!', '!', 'Threenager', ':', 'That', \"'s\", 'cause', 'you', 'are', 'growing', 'my', 'baby', '*', 'pats', 'my', 'belly', '*', 'Awe', '.', 'Love', 'my', 'kids', '!', '!'] 22322\n",
      "['I', 'wish', 'Kris', 'was', 'here', 'so', 'he', 'could', 'take', 'me', 'to', 'P.F', '.', 'Chang', \"'s\", 'again', '.', 'Ugh', '.', 'Yaaas', '.', '😍'] 22602\n",
      "['Ugh', '.', 'I', \"'m\", 'exhausted', '.', 'I', 'do', \"n't\", 'wan', 'na', 'get', 'ready', '.', '😣'] 22818\n",
      "['@', 'Chakratis', 'Ugh', '.', 'And', 'this', 'my', '3rd', 'time', 'doing', 'it', '...', 'apparently', 'it', 'never', 'gets', 'easier', '.'] 23006\n",
      "['Ugh', 'my', 'anxiety', 'is', 'through', 'the', 'roof', '😩'] 23464\n",
      "['Ugh', 'it', \"'s\", 'the', 'worst', 'watching', 'Brandon', 'leave', 'for', 'work', 'knowing', 'that', 'I', 'wo', \"n't\", 'see', 'him', 'until', 'the', 'morning', '!', '😭'] 23768\n",
      "['I', 'miss', 'Claudia', '.', '😭', 'got', 'ta', 'visit', 'her', 'soon', '.', 'Ugh', '.', 'Btw', ',', 'this', 'is', 'an', 'old', 'picture', '.', 'https', ':', '//t.co/MfFpmI6WNR'] 24431\n",
      "['Ugh', '#', 'GOT', 'spoilers', '😡'] 26022\n",
      "['@', 'JoseyEshelman', 'Ugh', ',', 'you', 'lucky', 'duck', '!'] 26675\n",
      "['Ugh', 'this', 'dog', 'keeps', 'barking', 'and', 'I', \"'m\", 'so', 'annoyed', '😤😤😤😤😤😤😤😤'] 26776\n",
      "['@', '_McKennaPaige', 'My', 'heart', '!', '!', '!', 'Ugh', 'I', 'love', 'this', 'tweet', 'so', 'much', '!', 'So', 'glad', 'he', 'got', 'to', 'hold', 'him', '!', '!', '❤️😍'] 27336\n",
      "['I', 'do', \"n't\", 'feel', 'good', 'tonight', '.', 'Ugh', '.', 'Might', 'go', 'to', 'sleep', 'early', '.', '😔😔'] 28077\n",
      "['They', 'want', 'me', 'to', 'have', 'two', 'baby', 'showers', ',', 'One', 'in', 'Miami', 'one', 'in', 'Gainesville', '.', 'Ugh', 'that', \"'s\", 'too', 'much', 'work'] 28279\n",
      "['Ugh', '.', 'Her', 'comes', 'the', 'ice', 'queen', 'to', 'hark', 'on', 'the', 'message', '.', '#', 'MarriedAtFirstSight', '#', 'MAFS'] 28656\n",
      "['Ugh', 'my', 'tummy', 'hurts', ':', '('] 28968\n",
      "['Ugh', ',', 'I', 'drove', '30', 'miles', 'away', 'from', 'Taco', 'Bell', 'and', 'then', 'realized', 'Taco', 'Bell', 'sounds', 'delicious', '.', '😑', 'I', \"'m\", 'coming', 'for', 'you', 'tomorrow', 'TB', '.'] 28992\n",
      "['Every', 'night', '.', 'Ugh', 'sometimes', 'I', 'feel', 'like', 'I', 'do', \"n't\", 'even', 'get', 'real', 'rest', '.', 'https', ':', '//t.co/MW4vpPY2dn'] 29689\n",
      "['Ugh', 'he', \"'s\", 'gorgeous', '😍😭🐾', 'http', ':', '//t.co/ZHNrYzQ35Q'] 29881\n",
      "['I', 'hate', 'my', 'supervisor', '.', 'Ugh', '.'] 31764\n",
      "['Ugh', 'why', 'did', 'justin', 'and', 'I', 'ever', 'move', 'out', 'of', 'our', 'house', '!'] 32114\n",
      "['@', 'sammarieestes', 'Ugh', 'no', ',', 'she', 'was', 'born', 'at', '3:51', 'after', '23', 'hours', 'of', 'labor', 'and', '40', 'mins', 'of', 'pushing', '😍😨😩'] 32577\n",
      "['Omg', 'Hunger', 'Games', 'blew', 'my', 'mind', '!', 'I', \"'m\", 'so', 'so', 'so', 'so', 'so', 'SOOOO', 'sad', 'it', \"'s\", 'over', '.', 'Ugh', '!', 'A', 'must', 'see', '😭'] 32648\n",
      "['Ugh', 'have', 'my', 'babies', 'https', ':', '//t.co/oTk427KvP3'] 33553\n",
      "['I', 'seriously', 'looked', 'so', 'adorable', 'back', 'in', 'the', 'day', '!', 'Ugh', '.', '😩', 'https', ':', '//t.co/b10rB56qpC'] 33573\n",
      "['Ugh', 'why', 'wo', \"n't\", 'ig', 'work', '!'] 33880\n",
      "['I', 'love', 'makeup', '&', 'amp', ';', 'shopping', 'and', 'even', 'tho', 'Jordan', 'hates', 'going', 'shopping', 'and', 'when', 'I', 'wear', 'makeup', 'he', 'said', 'he', \"'ll\", 'love', 'it', 'for', 'me', '.', 'Ugh', 'I', 'just', \"can't😩❤️\"] 34143\n",
      "['Ugh', 'I', \"'m\", 'too', 'lazy', 'to', 'even', 'go', 'somewhere', 'to', 'file', 'my', 'taxes', 'and', 'its', 'just', 'so', 'expensive', 'to', 'do', 'them', '.'] 34750\n",
      "['Do', \"n't\", 'want', 'to', 'do', 'anything', 'today', '.', '#', 'Ugh'] 34905\n",
      "['Ugh', 'I', 'miss', 'my', 'house', '😩'] 35071\n",
      "['When', 'your', 'life', 'is', 'just', 'so', 'stressful', '.', 'Ugh', 'fml', '.'] 35713\n",
      "['I', 'need', 'to', 'get', 'out', 'of', 'bed', '.', 'Ugh'] 36775\n",
      "['@', 'jenwilsonca', 'Yes', '.', 'Ugh', '.', 'Hate', 'that', 'feeling', '.'] 37102\n",
      "['Ugh', '.', 'My', 'feet', 'are', 'swollen', '.', 'My', 'back', 'hurts', '.', 'My', 'neck', 'hurts', '.', 'Im', 'ready', 'to', 'go', 'home', 'and', 'take', 'a', 'hot', 'bath', 'and', 'go', 'to', 'sleep', '.'] 37507\n",
      "['Ugh', '.', 'My', 'next', 'syringe', 'change', 'is', 'going', 'to', 'be', 'around', '4', 'am', '.', 'Whyyy', '.', '#', 'LifeWithAZofranPump'] 37840\n"
     ]
    }
   ],
   "source": [
    "ert = 'Ugh'\n",
    "# ert2 = 'Pump'\n",
    "for idx,i in enumerate(sentences_pred):\n",
    "    if ert in i:\n",
    "        print(i,idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "amateur-dover",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[6590] == error_sent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "planned-formula",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18952"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "considered-johnson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37664"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-petroleum",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
